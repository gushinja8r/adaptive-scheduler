{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run learners in job scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the learners\n",
    "\n",
    "We need the following variables:\n",
    "* `learners` a list of learners\n",
    "* `combos` a list of dicts of parameters that describe each learner\n",
    "* `fnames` a list of filenames of each learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile _learners.py\n",
    "\n",
    "import adaptive\n",
    "from functools import partial\n",
    "\n",
    "import funcs\n",
    "\n",
    "syst_pars = dict(a=4, L=40, r=20, shape=\"square\", dim=3)\n",
    "\n",
    "params = dict(g=50, mu=100, B_y=0, B_x=0, **funcs.constants_InAs)\n",
    "\n",
    "Ls = [1000, 2000, 3000, 5000, 10000]\n",
    "l_Rs = [200] #np.geomspace(30, 1000, 10).tolist() + [np.inf]\n",
    "l_es = [20, 50, 100, 200, 300, 500]\n",
    "rs = [25]\n",
    "\n",
    "combos = adaptive.utils.named_product(l_e=l_es, L=Ls, r=rs, l_R=l_Rs)\n",
    "\n",
    "learners = []\n",
    "fnames = []\n",
    "folder = \"data/q_phi_scaling_square_wire_new/\"\n",
    "for combo in combos:\n",
    "    f = partial(\n",
    "        funcs.conductance_1D,\n",
    "        x_name='B_z',\n",
    "        value_dict=combo,\n",
    "        syst_pars=syst_pars,\n",
    "        params=params,\n",
    "    )\n",
    "    learner = adaptive.AverageLearner1D(f, bounds=(0, 0.25))\n",
    "    learner.average_priority = 1\n",
    "    learner.min_seeds_per_point = 20\n",
    "    fnames.append(f\"{folder}_{combo}\")\n",
    "    learners.append(learner)\n",
    "\n",
    "learner = adaptive.BalancingLearner(learners, strategy='cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the previous code block and plot the learners\n",
    "from _learners import *\n",
    "adaptive.notebook_extension()\n",
    "learner.load(fnames)\n",
    "learner.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions for both the server (headnode) and the client (nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile slurm.py\n",
    "\n",
    "import getpass\n",
    "import subprocess\n",
    "import textwrap\n",
    "\n",
    "def make_sbatch(name, cores, executable=\"run_learner.py\", env=\"py37_min\"):\n",
    "    job_script = textwrap.dedent(\n",
    "        f\"\"\"\\\n",
    "        #!/bin/bash\n",
    "        #SBATCH --job-name {name}\n",
    "        #SBATCH --ntasks {cores}\n",
    "        #SBATCH --output {name}.out\n",
    "        #SBATCH --no-requeue\n",
    "\n",
    "        export MKL_NUM_THREADS=1\n",
    "        export OPENBLAS_NUM_THREADS=1\n",
    "        export OMP_NUM_THREADS=1\n",
    "\n",
    "        export MPI4PY_MAX_WORKERS=$SLURM_NTASKS\n",
    "        srun -n $SLURM_NTASKS --mpi=pmi2 ~/miniconda3/envs/{env}/bin/python3 -m mpi4py.futures {executable}\n",
    "        \"\"\"\n",
    "    )\n",
    "    return job_script\n",
    "\n",
    "\n",
    "def check_running(me_only=True):\n",
    "    cmd = [\n",
    "        \"/usr/bin/squeue\",\n",
    "        r'--Format=\",jobid:100,name:100,state:100,numnodes:100,reasonlist:400,\"',\n",
    "        \"--noheader\",\n",
    "        \"--array\",\n",
    "    ]\n",
    "    if me_only:\n",
    "        username = getpass.getuser()\n",
    "        cmd.append(f\"--user={username}\")\n",
    "    proc = subprocess.run(cmd, text=True, capture_output=True)\n",
    "    squeue = proc.stdout\n",
    "\n",
    "    if (\n",
    "        \"squeue: error\" in squeue\n",
    "        or \"slurm_load_jobs error\" in squeue\n",
    "        or proc.returncode != 0\n",
    "    ):\n",
    "        raise RuntimeError(\"SLURM is too busy.\")\n",
    "\n",
    "    squeue = [line.split() for line in squeue.split(\"\\n\")]\n",
    "    squeue = [line for line in squeue if line]\n",
    "    allowed = (\"PENDING\", \"RUNNING\")\n",
    "    running = {\n",
    "        job_id: dict(\n",
    "            job_name=job_name,\n",
    "            state=state,\n",
    "            n_nodes=int(n_nodes),\n",
    "            reason_list=reason_list,\n",
    "        )\n",
    "        for job_id, job_name, state, n_nodes, reason_list in squeue\n",
    "        if state in allowed\n",
    "    }\n",
    "    return running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile server_support.py\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "from tinydb import TinyDB, Query\n",
    "import zmq\n",
    "import zmq.asyncio\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "from slurm import make_sbatch, check_running\n",
    "\n",
    "\n",
    "ctx = zmq.asyncio.Context()\n",
    "\n",
    "\n",
    "def dispatch(request, db_fname):\n",
    "    request_type, request_arg = request\n",
    "\n",
    "    if request_type == \"start\":\n",
    "        job_id = request_arg  # workers send us their slurm ID for us to fill in\n",
    "        # give the worker a job and send back the fname and combo to the worker\n",
    "        return choose_combo(db_fname, job_id)\n",
    "\n",
    "    elif request_type == \"stop\":\n",
    "        fname = request_arg  # workers send us the fname they were given\n",
    "        return done_with_learner(db_fname, fname)  # reset the job_id to None\n",
    "\n",
    "    else:\n",
    "        print(f\"unknown request type: {request_type}\")\n",
    "\n",
    "\n",
    "async def manage_database(address, db_fname):\n",
    "    socket = ctx.socket(zmq.REP)\n",
    "    socket.bind(address)\n",
    "    try:\n",
    "        while True:\n",
    "            request = await socket.recv_pyobj()\n",
    "            reply = dispatch(request, db_fname)\n",
    "            await socket.send_pyobj(reply)\n",
    "    finally:\n",
    "        socket.close()\n",
    "\n",
    "\n",
    "async def manage_jobs(job_names, db_fname, ioloop, cores=8, interval=30):\n",
    "    with ProcessPoolExecutor() as ex:\n",
    "        while True:\n",
    "            running = check_running()\n",
    "            update_db(db_fname, running)  # in case some jobs died\n",
    "            running_job_names = {job[\"job_name\"] for job in running.values()}\n",
    "            for job_name in job_names:\n",
    "                if job_name not in running_job_names:\n",
    "                    await ioloop.run_in_executor(ex, start_job, job_name, cores)\n",
    "            await asyncio.sleep(interval)\n",
    "\n",
    "\n",
    "def create_empty_db(db_fname, fnames, combos):\n",
    "    entries = [\n",
    "        dict(fname=fname, combo=combo, job_id=None, is_done=False)\n",
    "        for fname, combo in zip(fnames, combos)\n",
    "    ]\n",
    "    if os.path.exists(db_fname):\n",
    "        os.remove(db_fname)\n",
    "    with TinyDB(db_fname) as db:\n",
    "        db.insert_multiple(entries)\n",
    "\n",
    "\n",
    "def update_db(db_fname, running):\n",
    "    \"\"\"If the job_id isn't running anymore, replace it with None.\"\"\"\n",
    "    with TinyDB(db_fname) as db:\n",
    "        doc_ids = [entry.doc_id for entry in db.all() if entry[\"job_id\"] not in running]\n",
    "        db.update({\"job_id\": None}, doc_ids=doc_ids)\n",
    "\n",
    "\n",
    "def choose_combo(db_fname, job_id):\n",
    "    Entry = Query()\n",
    "    with TinyDB(db_fname) as db:\n",
    "        entry = db.get(Entry.job_id == None)\n",
    "        db.update({\"job_id\": job_id}, doc_ids=[entry.doc_id])\n",
    "    return entry[\"fname\"], entry[\"combo\"]\n",
    "\n",
    "\n",
    "def done_with_learner(db_fname, fname):\n",
    "    Entry = Query()\n",
    "    with TinyDB(db_fname) as db:\n",
    "        db.update({\"job_id\": None, \"is_done\": True}, Entry.fname == fname)\n",
    "\n",
    "\n",
    "\n",
    "def start_job(name, cores=8, *, job_script_function=make_sbatch):\n",
    "    with open(name + \".sbatch\", \"w\") as f:\n",
    "        job_script = job_script_function(name, cores)\n",
    "        f.write(job_script)\n",
    "\n",
    "    returncode = None\n",
    "    while returncode != 0:\n",
    "        returncode = subprocess.run(\n",
    "            f\"sbatch {name}.sbatch\".split(), stderr=subprocess.PIPE\n",
    "        ).returncode\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile client_support.py\n",
    "\n",
    "import os\n",
    "import zmq\n",
    "\n",
    "from _learners import learners, combos\n",
    "\n",
    "ctx = zmq.Context()\n",
    "\n",
    "def get_learner(url):\n",
    "    with ctx.socket(zmq.REQ) as socket:\n",
    "        socket.connect(url)\n",
    "        job_id = os.environ.get(\"SLURM_JOB_ID\", \"UNKNOWN\")\n",
    "        socket.send_pyobj((\"start\", job_id))\n",
    "        fname, combo = socket.recv_pyobj()\n",
    "    learner = next(lrn for lrn, c in zip(learners, combos) if c == combo)\n",
    "    return learner, fname\n",
    "\n",
    "\n",
    "def tell_done(url, fname):\n",
    "    with ctx.socket(zmq.REQ) as socket:\n",
    "        socket.connect(url)\n",
    "        socket.send_pyobj((\"stop\", fname))\n",
    "        socket.recv_pyobj()  # Needed because of socket type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Python script that is being run in the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to use the headnode's IP below.\n",
    "import socket\n",
    "import zmq.ssh\n",
    "ip = socket.gethostbyname(socket.gethostname())\n",
    "port = zmq.ssh.tunnel.select_random_ports(1)[0]\n",
    "print(f'tcp://{ip}:{port}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_learner.py\n",
    "\n",
    "import adaptive\n",
    "from mpi4py.futures import MPIPoolExecutor\n",
    "\n",
    "import client_support\n",
    "\n",
    "url = \"tcp://10.76.0.5:57681\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    learner, fname = client_support.get_learner(url)\n",
    "    learner.load(fname)\n",
    "    ex = MPIPoolExecutor()\n",
    "    runner = adaptive.Runner(\n",
    "        learner,\n",
    "        executor=ex,\n",
    "        goal=None,\n",
    "        shutdown_executor=True,\n",
    "        ioloop=None,\n",
    "        retries=10,\n",
    "        raise_if_retries_exceeded=False,\n",
    "    )\n",
    "    runner.start_periodic_saving(dict(fname=fname), interval=600)\n",
    "    runner.ioloop.run_until_complete(runner.task)  # wait until runner goal reached\n",
    "    client_support.is_done(url, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the files that were created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from importlib import reload\n",
    "\n",
    "from pprint import pprint\n",
    "from tinydb import TinyDB\n",
    "\n",
    "import server_support, _learners, run_learner\n",
    "\n",
    "reload(server_support)\n",
    "reload(_learners)\n",
    "reload(run_learner)\n",
    "\n",
    "db_fname = 'running.tinydb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new database\n",
    "server_support.create_empty_db(db_fname, _learners.fnames, _learners.combos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the running learners\n",
    "All the onces that are `None` are still `PENDING` or are not scheduled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with TinyDB(db_fname) as db:\n",
    "    pprint(db.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the job scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some unique names for the jobs\n",
    "job_names = [f\"WAL-{i}\" for i in range(len(_learners.learners))]\n",
    "\n",
    "ioloop = asyncio.get_event_loop()\n",
    "\n",
    "database_task = ioloop.create_task(\n",
    "    server_support.manage_database(\"tcp://*:57681\", db_fname)\n",
    ")\n",
    "\n",
    "job_task = ioloop.create_task(\n",
    "    server_support.manage_jobs(job_names, db_fname, ioloop, cores=50*8, interval=60)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_task.cancel(), database_task.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_task.print_stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_task.print_stack()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
