{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run learners in job scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the learners\n",
    "\n",
    "We need the following variables:\n",
    "* `learners` a list of learners\n",
    "* `fnames` a list of file names, one for each learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile learners_file.py\n",
    "\n",
    "import adaptive\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def h(x, offset=0):\n",
    "    import numpy as np\n",
    "    import random\n",
    "\n",
    "    for _ in range(10):  # Burn some CPU time just because\n",
    "        np.linalg.eig(np.random.rand(1000, 1000))\n",
    "\n",
    "    a = 0.01\n",
    "    return x + a ** 2 / (a ** 2 + (x - offset) ** 2)\n",
    "\n",
    "\n",
    "offset = [i / 20 - 0.5 for i in range(20)]\n",
    "\n",
    "combos = adaptive.utils.named_product(offset=offset)\n",
    "\n",
    "learners = []\n",
    "fnames = []\n",
    "\n",
    "for i, combo in enumerate(combos):\n",
    "    f = partial(h, offset=combo[\"offset\"])\n",
    "    learner = adaptive.Learner1D(f, bounds=(-1, 1))\n",
    "    fnames.append(f\"data/{combo}\")\n",
    "    learners.append(learner)\n",
    "\n",
    "learner = adaptive.BalancingLearner(learners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the previous code block and plot the learners\n",
    "from learners_file import *\n",
    "adaptive.notebook_extension()\n",
    "learner.load(fnames)\n",
    "learner.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1, the simple way\n",
    "\n",
    "After defining the `learners` and `fnames` in an file (above) we can start to run these learners.\n",
    "\n",
    "We split up all learners into seperate jobs, all you need to do is to specify how many cores per job you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import adaptive_scheduler\n",
    "\n",
    "def goal(learner):\n",
    "    return learner.npoints > 200\n",
    "\n",
    "run_manager = adaptive_scheduler.server_support.RunManager(\n",
    "    learners_file=\"learners_file.py\",\n",
    "    goal=goal,\n",
    "    cores_per_job=12,\n",
    "    log_interval=30,\n",
    "    save_interval=30,\n",
    ")\n",
    "run_manager.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the current queue with\n",
    "adaptive_scheduler.slurm.queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the logfiles and put it in a `pandas.DataFrame`.\n",
    "# This only returns something when there are log-files to parse!\n",
    "# So after `run_manager.log_interval` has passed.\n",
    "run_manager.parse_log_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the database\n",
    "run_manager.get_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to STOP managing the database and jobs\n",
    "run_manager.cancel(), run_manager.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2, the manual way \n",
    "\n",
    "The `adaptive_scheduler.server_support.RunManager` above essentially does everything we do below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Python script that is run on the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to use the headnode's address in the next cell\n",
    "from adaptive_scheduler import server_support\n",
    "server_support.get_allowed_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_learner.py\n",
    "\n",
    "import adaptive\n",
    "from adaptive_scheduler import client_support\n",
    "from mpi4py.futures import MPIPoolExecutor\n",
    "\n",
    "from learners_file import learners, fnames\n",
    "\n",
    "if __name__ == \"__main__\":  # ‚Üê use this, see warning @ https://bit.ly/2HAk0GG\n",
    "    url = \"tcp://10.75.0.5:57101\"\n",
    "    learner, fname = client_support.get_learner(url, learners, fnames)\n",
    "    learner.load(fname)\n",
    "    runner = adaptive.Runner(\n",
    "        learner, executor=MPIPoolExecutor(), shutdown_executor=True, goal=None\n",
    "    )\n",
    "    runner.start_periodic_saving(dict(fname=fname), interval=600)\n",
    "    client_support.log_info(runner, interval=600)  # log info in the job output script\n",
    "    runner.ioloop.run_until_complete(runner.task)  # wait until runner goal reached\n",
    "    client_support.tell_done(url, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptive_scheduler import server_support\n",
    "from learners_file import learners, fnames\n",
    "\n",
    "db_fname = 'running.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_support.create_empty_db(db_fname, fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the running learners in the database\n",
    "All the ones that are `None` are still `PENDING`, reached their goal, or are not scheduled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_support.get_database(db_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the job scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from adaptive_scheduler import server_support, slurm\n",
    "from learners_file import learners, fnames\n",
    "\n",
    "# create unique names for the jobs\n",
    "job_names = [f\"test-job-{i}\" for i in range(len(learners))]\n",
    "\n",
    "# start the \"job manager\" and the \"database manager\"\n",
    "database_task = server_support.start_database_manager(\"tcp://10.75.0.5:57101\", db_fname)\n",
    "\n",
    "job_task = server_support.start_job_manager(\n",
    "    job_names,\n",
    "    db_fname=db_fname,\n",
    "    cores=2,\n",
    "    interval=60,\n",
    "    run_script=\"run_learner.py\",  # optional\n",
    "    job_script_function=slurm.make_job_script,  # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_task.print_stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_task.print_stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to STOP managing the database and jobs\n",
    "from adaptive_scheduler import cancel_jobs\n",
    "job_task.cancel(), database_task.cancel(), cancel_jobs(job_names)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
