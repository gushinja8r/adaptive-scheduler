{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive-scheduler example\n",
    "\n",
    "[Read the documentation](https://adaptive-scheduler.readthedocs.io/en/latest/#what-is-this) to see what this is all about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: define the simulation\n",
    "\n",
    "Often one wants to sweep a continuous 1D or 2D space for multiple parameters. [Adaptive](http://adaptive.readthedocs.io) is the ideal program to do this. We define a simulation by creating several `adaptive.Learners`. \n",
    "\n",
    "We **need** to define the following variables:\n",
    "* `learners` a list of learners\n",
    "* `fnames` a list of file names, one for each learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile learners_file.py\n",
    "\n",
    "import adaptive\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def h(x, width=0.01, offset=0):\n",
    "    import numpy as np\n",
    "    import random\n",
    "\n",
    "    for _ in range(10):  # Burn some CPU time just because\n",
    "        np.linalg.eig(np.random.rand(1000, 1000))\n",
    "\n",
    "    a = width\n",
    "    return x + a ** 2 / (a ** 2 + (x - offset) ** 2)\n",
    "\n",
    "\n",
    "offsets = [i / 10 - 0.5 for i in range(10)]\n",
    "\n",
    "combos = adaptive.utils.named_product(offset=offsets, width=[0.01, 0.05])\n",
    "\n",
    "learners = []\n",
    "fnames = []\n",
    "\n",
    "for combo in combos:\n",
    "    f = partial(h, **combo)\n",
    "    learner = adaptive.Learner1D(f, bounds=(-1, 1))\n",
    "    fnames.append(f\"data/{combo}\")\n",
    "    learners.append(learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 (option 1): the simple way\n",
    "\n",
    "After defining the `learners` and `fnames` in an file (above) we can start to run these learners.\n",
    "\n",
    "We split up all learners into seperate jobs, all you need to do is to specify how many cores per job you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import adaptive_scheduler\n",
    "\n",
    "def goal(learner):\n",
    "    return learner.npoints > 200\n",
    "\n",
    "run_manager = adaptive_scheduler.server_support.RunManager(\n",
    "    learners_file=\"learners_file.py\",\n",
    "    goal=goal,\n",
    "    cores_per_job=12,\n",
    "    log_interval=30,\n",
    "    save_interval=30,\n",
    ")\n",
    "run_manager.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the current queue with\n",
    "import pandas as pd\n",
    "queue = adaptive_scheduler.slurm.queue()\n",
    "df = pd.DataFrame(queue).transpose()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the logfiles and put it in a `pandas.DataFrame`.\n",
    "# This only returns something when there are log-files to parse!\n",
    "# So after `run_manager.log_interval` has passed.\n",
    "df = run_manager.parse_log_files()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the database\n",
    "df = pd.DataFrame(run_manager.get_database())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the calculation started and some data has been saved, we can display the learners\n",
    "import adaptive\n",
    "from learners_file import learners, fnames, combos\n",
    "from adaptive_scheduler.utils import load_parallel\n",
    "adaptive.notebook_extension()\n",
    "load_parallel(learners, fnames)\n",
    "\n",
    "learner = adaptive.BalancingLearner(learners, cdims=combos)\n",
    "learner.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended example\n",
    "Sometimes you cannot formulate your problem with Adaptive, instead you just want to run a function as a sequence of parameters.\n",
    "\n",
    "Surprisingly, this approach with a `SequenceLearner` [is slightly faster than `ipyparallel.Client.map`](https://github.com/python-adaptive/adaptive/pull/193#issuecomment-491062073)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile learners_file_sequence.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from adaptive_scheduler.sequence_learner import SequenceLearner # https://github.com/python-adaptive/adaptive/pull/193\n",
    "from adaptive_scheduler.utils import split, combo_to_fname\n",
    "from adaptive.utils import named_product\n",
    "\n",
    "\n",
    "def g(combo):\n",
    "    combo = dict(combo)  # the sequence learner passes dicts as tuples\n",
    "    x, y, z = combo['x'], combo['y'], combo['z']\n",
    "\n",
    "    for _ in range(5):  # Burn some CPU time just because\n",
    "        np.linalg.eig(np.random.rand(1000, 1000))\n",
    "\n",
    "    return x ** 2 + y ** 2 + z ** 2\n",
    "\n",
    "\n",
    "combos = named_product(x=np.linspace(0, 10), y=np.linspace(-1, 1), z=np.linspace(-3, 3))\n",
    "\n",
    "print(f\"Length of combos: {len(combos)}.\")\n",
    "\n",
    "\n",
    "# We could run this as 1 job with N nodes, but we can also split it up in multiple jobs.\n",
    "# This is desireable when you don't want to run a single job with 300 nodes for example.\n",
    "njobs = 100\n",
    "split_combos = list(split(combos, njobs))\n",
    "\n",
    "print(f\"Length of split_combos: {len(split_combos)} and length of split_combos[0]: {len(split_combos[0])}.\")\n",
    "\n",
    "learners, fnames = [], []\n",
    "learners = [SequenceLearner(g, combos_part) for combos_part in split_combos]\n",
    "fnames = [combo_to_fname(combos_part[0], folder=\"data\") for combos_part in split_combos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now start the `RunManager` with a lot of arguments to showcase some of the options you can use to customize your run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import adaptive_scheduler\n",
    "\n",
    "job_script = partial(\n",
    "    adaptive_scheduler.slurm.make_job_script,\n",
    "    executor_type=\"ipyparallel\",\n",
    "    extra_sbatch=[\"--exclusive\", \"--time=24:00:00\"],\n",
    "    extra_env_vars=[\"PYTHONPATH='my_dir:$PYTHONPATH'\"],\n",
    ")\n",
    "\n",
    "\n",
    "def goal(learner):\n",
    "    return learner.done()  # the standard goal for a SequenceLearner\n",
    "\n",
    "\n",
    "run_manager2 = adaptive_scheduler.server_support.RunManager(\n",
    "    goal=goal,\n",
    "    cores_per_job=24,\n",
    "    log_interval=10,\n",
    "    save_interval=30,\n",
    "    runner_kwargs=dict(retries=5, raise_if_retries_exceeded=False),\n",
    "    job_script_function=job_script,\n",
    "    executor_type=\"ipyparallel\",\n",
    "    kill_on_error=\"srun: error:\",  # cancel a job if this is inside a log\n",
    "    log_file_folder=\"logs\",\n",
    "    learners_file=\"learners_file_sequence.py\",  # the file that has `learners` and `fnames`\n",
    "    job_name=\"example-sequence\",  # this is used to generate unqiue job names\n",
    "    db_fname=\"example-sequence.json\",  # the database keeps track of job_id <-> (learner, is_done)\n",
    "    start_job_manager_kwargs=dict(\n",
    "        max_fails_per_job=10,  # the RunManager is cancelled after njobs * 10 fails\n",
    "        max_simultaneous_jobs=300,  # limit the amount of simultaneous jobs\n",
    "        python_executable=\"/escratch/home/t-banij/miniconda3/envs/py37/bin/python\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_manager2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_manager2.parse_log_files()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learners_file_sequence import learners, fnames, combos\n",
    "import adaptive\n",
    "from adaptive_scheduler.utils import load_parallel\n",
    "load_parallel(learners, fnames)\n",
    "result = sum([l.result() for l in learners], [])  # combine all learner's result into 1 list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 (option 2): the manual way \n",
    "\n",
    "The `adaptive_scheduler.server_support.RunManager` above essentially does everything we do below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Python script that is run on the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to use the headnode's address in the next cell\n",
    "from adaptive_scheduler import server_support\n",
    "server_support.get_allowed_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_learner.py\n",
    "\n",
    "import adaptive\n",
    "from adaptive_scheduler import client_support\n",
    "from mpi4py.futures import MPIPoolExecutor\n",
    "\n",
    "from learners_file import learners, fnames\n",
    "\n",
    "if __name__ == \"__main__\":  # ← use this, see warning @ https://bit.ly/2HAk0GG\n",
    "    url = \"tcp://10.75.0.5:57101\"\n",
    "    learner, fname = client_support.get_learner(url, learners, fnames)\n",
    "    learner.load(fname)\n",
    "    runner = adaptive.Runner(\n",
    "        learner, executor=MPIPoolExecutor(), shutdown_executor=True, goal=None\n",
    "    )\n",
    "    runner.start_periodic_saving(dict(fname=fname), interval=600)\n",
    "    client_support.log_info(runner, interval=600)  # log info in the job output script\n",
    "    runner.ioloop.run_until_complete(runner.task)  # wait until runner goal reached\n",
    "    client_support.tell_done(url, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptive_scheduler import server_support\n",
    "from learners_file import learners, fnames\n",
    "\n",
    "db_fname = 'running.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_support.create_empty_db(db_fname, fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the running learners in the database\n",
    "All the ones that are `None` are still `PENDING`, reached their goal, or are not scheduled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_support.get_database(db_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the job scripts with the `job_manager` and `database_manager`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from adaptive_scheduler import server_support, slurm\n",
    "from learners_file import learners, fnames\n",
    "\n",
    "# create unique names for the jobs\n",
    "job_names = [f\"test-job-{i}\" for i in range(len(learners))]\n",
    "\n",
    "# start the \"job manager\" and the \"database manager\"\n",
    "database_task = server_support.start_database_manager(\"tcp://10.75.0.5:57101\", db_fname)\n",
    "\n",
    "job_task = server_support.start_job_manager(\n",
    "    job_names,\n",
    "    db_fname=db_fname,\n",
    "    cores=2,\n",
    "    interval=60,\n",
    "    run_script=\"run_learner.py\",  # optional\n",
    "    job_script_function=slurm.make_job_script,  # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_task.print_stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_task.print_stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to STOP managing the database and jobs\n",
    "from adaptive_scheduler import cancel_jobs\n",
    "job_task.cancel(), database_task.cancel(), cancel_jobs(job_names)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
